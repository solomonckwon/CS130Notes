\documentclass{article}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage{outline}
\usepackage[margin=1in]{geometry}

\title{CS130 Notes}
\author{Solomon Kwon}

\setlength{\parindent}{0pt}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\section{Source Control}

\subsubsection*{Allows for}
\begin{itemize}
    \item Reproducible state
    \item Backing up work
    \item Documenting progress
    \item Post-mortem
    \item Make collaboration possible
    \item Legal pedigree
\end{itemize}

\vspace*{1em}

\subsubsection*{Adds complexity:}

\begin{itemize}
    \item Can't just edit files willy-nilly
    \item Have to check them in and check them out.
    \item Have to figure out how to size changelists
    \item More thinking
\end{itemize}
\begin{multicols}{2}
    \subsubsection*{... but reduces risk}
        \begin{itemize}
            \item of losing progress
            \item of not being able to solve user problems
            \item of legal liability
        \end{itemize}
    \subsubsection*{... and is more scalable}
        \begin{itemize}
            \item more people can contribute sensibly
            \item allows specialization
        \end{itemize}
\end{multicols}

\subsection{Source Control Tools}

\begin{itemize}
    \item Git
    \item Subversion
    \item Mercurial
    \item CVS
    \item Perforce
    \item SourceSafe
    \item BitKeeper
\end{itemize}

\subsection{Changelists and filesets}

Files in source control organized by the logic of the project and build system.

\vspace*{1em}

Different source control systems treat files differently.
\begin{itemize}
    \item Git treats the entire repository at once, and references the whole fileset by hash
\end{itemize}
    
Changelists are determined by development logic:
\begin{itemize}
    \item Chosen to advance the goals of the project
    \item Use changelists to make sense internally as a project state transition\par
    Ideally: Self-contained, small, single-function
    \item Can be part of a larger group of changelists
    \item Larger sequences can be arranged as branches
\end{itemize}

\subsection{Git source control}

\subsubsection{Diffs}
Instead of storing a copy of the repo at each version, store only a \textbf{diff} that tells you how to modify each version to get to the next incremental version.

\includegraphics*[width=\linewidth]{gitDiff.png}

\includegraphics*[width=\linewidth]{gitDIffEx.png}

\textbf{Commit:} A snapshot of the repository at a given point in time. Think "node"

\textbf{Changelist: } A diff that represents how the repository is changed between commits. Think "edge"

\subsubsection{Making new commits}

In order to commit
\begin{enumerate}
    \item Edit files locally
    \item Add them to potential commit
    \item Call `git commit`
\end{enumerate}

Git will automatically collapse your changes into a diff and store those changes. \par
- Git will also give your commit a name (hash of fileset)

\subsubsection{Making a new branch}

\textbf{Branch: } A pointer to a specific commit.

"Making a new branch" means "create a new pointer that points at the same place I'm currently pointed at"

\vspace*{1em}

`git checkout -b new-feature`\par
- creates new branch 'new-feature'

\vspace*{1em}

\textbf{HEAD: } The commit where I am \textit{currently} working. Indicated by asterisks to annotate where HEAD is pointing.

\includegraphics*[width=\linewidth]{gitBranch.png}

'git checkout some\_branch'\par
- moves pointer to some\_branch

\subsubsection{Tree turns into graph}

\textbf{Merge:} Create a commit with 2 parents

\includegraphics*[width=\linewidth]{gitMerge.png}
\begin{center}
    Whats in v7? [v5 plus Diff 5] OR [v6 plus Diff 3 plus Diff 4]

    Both are equivalent...
\end{center}

\subsubsection{Pruning the tree}

\includegraphics*[width=\linewidth]{gitPruningTree.png}

\pagebreak

\section{Testing}

"The software should be subjected to extensive testing and formal analysis at the module and software level; system testing alone is not adequate."

\vspace*{1em}

Good tests are:
\begin{itemize}
    \item Automated
    \item Repeatable
    \item Meaningful
    \item Reliable
    \item Easy to run
    \item Fast 
    \item Checked into revision control
\end{itemize}

Types of tests:
\begin{multicols}{2}
    \subsubsection*{Types of Tests}
        \begin{itemize}
            \item Unit Tests
            \item Integration Tests
            \item Regression Tests
            \item ect.....
        \end{itemize}
    \subsubsection*{}
        \vspace*{1em}
        \includegraphics*[width=\linewidth]{testTypes.png}
\end{multicols}

Well tested system has many tests, of many kinds, which provide overlapping coverage.

\subsection{Test Driven Development}
WHen you know the desired result before you know how to implement, you can write unit tests that are examples of using your API.

\vspace*{1em}

Before fixing a bug, write a unit test that fails because of the bug.

\subsection{Signs you need MORE tests}

\begin{itemize}
    \item "Low-level"
    \item "Mission-critical"
    \item Lots of people are depending on it (customers, coworkers)
    \item Failure is expensive (pathfinder)
    \item Failure will result in death (Therac-25)
\end{itemize}

\textbf{Write more tests} isn't always the answer....

\begin{itemize}
    \item Prototypes and rapidly changing code 
    \item One-off scripts, non-productive code
    \item Tests can be flakey 
    \item Sometimes there is no substitute for manual testing
    User interfaces, Hardware("dogfooding")
\end{itemize}

Tests are expensive to write and maintain. Time, money, opportunity.

\includegraphics*[width=0.6\linewidth]{testChecklist.png}

General guidelines for course:
\begin{itemize}
    \item Code should have unit tests, at least for basic use cases and common errors
    \item Pretty much all classes and source files should have unit test
    \item Binaries should have an integration test 
\end{itemize}

\subsection{What to test?}

\begin{itemize}
    \item Public API, typical use cases first
    \item Error handling
    \item Anything complex 
\end{itemize}

\textbf{Unit tests: } Tests units of code (classes, files, modules) in isolation.
\begin{itemize}
    \item Written in the same language as the code they test.
    \item High bang-for-the-buck.
    Fast to write + fast to execute
    \item Serve as documentation, demonstrates the API
    \item You can make changes quickly and confidently.
    \item Faster in the long run.
\end{itemize}

\subsubsection*{Finding good test cases}
\begin{itemize}
    \item Boundary conditions\par
    int Add(int a, int b) -> What if a, b are 0, 1 negative, really big, ect.\par
    double Average(vector<double> a) -> What if a is empty, 1 element, really big, ect.
    \item Pre and post-conditions\par
    What should be true before and after a function, loop, or conditional?
    \item Be defensive\par
    What happens if something that "can't happen" does?
    \item Error conditions\par
    What happens in the case of an error? (e.g. file not found)\par
    Do we crash, return early, throw an exception, ect..
\end{itemize}

When in doubt: write your API docs on public methods. For each public method, write a test case for each statement of the docs.

\subsubsection*{Things that are hard to unit test}
\begin{itemize}
    \item Global variables
    \item Long, complex methods
    \item Objects with state
    \item Tight coupling (interdependent classes)
    \item Bad abstractions
\end{itemize}


\subsection{Dependency Injections}

DI is a design pattern used in software development to enhance modularity, maintainability, and testability of code.

\vspace*{1em}

Objects \textbf{ask} for what they need instead of \textbf{retrieving} what they need.

In this way, your objects will depend solely on interfaces, and will be \textbf{implementation agnostic}.

\includegraphics*[width=\linewidth]{DIEx1.png}
\includegraphics*[width=\linewidth]{DIFramwork.png}

\subsubsection*{One weird thing...}

Adoption of a DI framework will cause your code to look pretty different. E.g. Google codebase -> No usage of 'new' anywhere!

\begin{center}
    \includegraphics*[width=0.5\linewidth]{weirdDI.png}
\end{center}

\subsection{Mocks vs Fakes}

\textbf{Real object: } Same implementation you'd use in production.

\textbf{Fake object: } Trivial implementation of the interface that satisfies all contracts but relies on memory only (e.g. FakeDatabase performs read/mutate operations as expected, but on an array in memory instead of using a SQL database)

\textbf{Mock object: } Placeholder test object that can be set up to respond to respond to specific stimuli and report back how it was interacted with.

\vspace*{1em}

Real object should be preferred for testing:
\begin{itemize}
    \item Mocks/fakes have to be written.
    \item Mocks/fakes could have bugs. 
    \item Mocks/fakes have more maintenance cost.
\end{itemize}

Mock objects are also an option:
\begin{itemize}
    \item Pros
    \begin{itemize}
        \item Only need to define the behavior you care about in your test
        \item Mocks can confirm some API contracts
    \end{itemize}
    \item Cons
    \begin{itemize}
        \item Maintenance cost
        \item Eay to misuse
    \end{itemize}
\end{itemize}

Fake objects are usually the next best thing to real objects:
\begin{itemize}
    \item Pros 
    \begin{itemize}
        \item Only need to define it once. Service owner should also own and maintain the fake for their service.
        \item Fake can be tested independently to verify its behavior.
    \end{itemize}
    \item Cons 
    \begin{itemize}
        \item Maintenance cost
        \item Sometimes not trivial to satisfy the service's contract in memory. Think about how you would write the minimal implementation of Socket that satisfied its API contracts.
    \end{itemize}
\end{itemize}

\subsection{Testing State vs Testing Interaction}

Best practice is to make sure your testing state is to \textbf{test your public APIs}.

Your public API should have a clear contract that defines state that should be returned by your service, so your API documentation can serve as a blueprint of what to test.

\vspace*{1em}

A bad code smell is testing package private methods/making a method visible specifically for testing.

This almost always indicates you're testing some sort of interaction, since your test is now relying on implementation details.

\subsection{Testing Readability}

\includegraphics*[width=0.5\linewidth]{testStructure.png}

Tests are not tested, so err on the side of repetitive and verbose test. Refactoring into concise helper methods is best practice in production code, but can introduce untested programming errors when used in tests.

\vspace*{1em}

Testing is often great documentation. How often do you look for usages of an API to figure out how it works?

\includegraphics*[width=\linewidth]{testingDRYDAMP.png}

\subsection{Integration Tests}

At this point, we've refactored all our code to make unit testing easy. We've stubbed out all of the really hairy stuff (network, disk, etc.). 

\vspace*{1em}

How do we test the connections we stubbed out?

By testing code end to end.

\begin{itemize}
    \item Integration tests tend do be VERY expensive
    \begin{itemize}
        \item to write
        \item to keep green
        \item to run
    \end{itemize}
\end{itemize}

\subsubsection*{Picking integration tests}

Pick a small handful of CRITICAL use cases for your application, and test those. 

\vspace*{1em}

Want to be \textbf{100\%} sure that if key functionality in app is broken, then you know about it.

\subsubsection*{Web Driver Integration Ex}

\includegraphics*[width=\linewidth]{integrationTestEx.png}

Test your API by firing a variety of real requests.

Keep a copy of "golden" response for each request and make sure the requests coming back match it in all ways that matter

\includegraphics*[width=\linewidth]{integrationScreenshot.png}

\pagebreak

\section{Code Reviews}

The best proof of reusability is review. Code review ensures that someone else values and understands the code.

\begin{itemize}
    \item Give it to someone else and see if they appreciate it.
    \item They should be able to merge it into their stack and use it.
\end{itemize}

\subsubsection*{Thinking Systems}
\textbf{System 1:} Intuitive, fast, but has higher error rates

\textbf{System 2:} Methodical, slow, low error rates, but is hard to engage

\vspace*{1em}

Code review makes us more likely to generate "system 2" code.

\vspace*{1em}

Research on code review shows:
\begin{itemize}
    \item Code review catches 60-90\% of errors. Fagan 1976
    \item The first reviewer and first review matter the most. Cohen 2006
    \item Defect rates in code are related to program size, and seemingly little else. El Imam 2001
\end{itemize}

\vspace*{1em}

Before sending out code for review make sure you have done these things:
\begin{enumerate}
    \item Write code that is easy to review!
    \item Keep changes small and focused
    \item Send a work in progress review out early
    \item Review your own work!
\end{enumerate}

\subsection{Change Descriptions}
Change descriptions should be more than just \textit{what} the change is. \textit{Why} was the change made? \textit{How} was the \textit{why} accomplished? Any new testing?

\vspace*{1em}

\subsection{During code review}

Flag errors of execution:
\begin{itemize}
    \item Unclear documentation
    \item Typos
    \item Style Violations
    \item Bad/missing Tests
    \item Bugs
\end{itemize}

Apply deliberate thinking to find errors:
\begin{itemize}
    \item Is this algorithm correct?
    \item Is this built to specifications?
    \item Does this code need to exist?
    \item Is this the most elegant solution?
\end{itemize}

\vspace*{1em}

Code review also develops a shared understanding about the purpose of the code. 
\begin{itemize}
    \item Aligns the team on "landmarks"
    \item Small changes can lead to target drift
    \item Each code review is an opportunity to course-correct
    \item How will this code be used next year?
\end{itemize}

It also establishes N+1 availability in understanding of the code, which is important because teams are dynamic.

\subsection{Methods of code review}
\begin{itemize}
    \item Projecting code in a meeting
    \item Pair programming
    \item Pull requests
    \item Code review tools
\end{itemize}

\textbf{Remember:} Be thoughtful and careful with words. Avoid personal attacks, reviews are not a competition.

\subsection{Git with Gerrit}

\subsubsection*{Sending Code for Review}

Before starting work on a feature, you should create a new git branch, assuming that the code does not depend on anything outside of the main branch. 

Features should be small enough that you make a small number of commits before sending code out for review.

First create a branch:
\begin{verbatim}
    $ git checkout -b my_feature 
\end{verbatim}

...edit code then:
\begin{verbatim}
    $ git add .
    $ git commit -m "Wrote a bunch of code"
    $ git review --reviewers my_reviewer
\end{verbatim}

\subsubsection*{Responding to reviews }

You can use git review to pull your latest patch set for a given change ID from the server and create a new temporary branch in which you can make all your edits. 

When you're done, you can amend your previous commit, then upload the patch to your review thread. 

\vspace*{1em}

First view your changes and get a change ID:

\begin{verbatim}
    $ git review -l
    $ git review -d my_change_id
\end{verbatim}

...edit your code, then:

\begin{verbatim}
    $ git add .
    $ git commit --amend --no-edit
    $ git review -f 
\end{verbatim}

\textbf{Note: } it's important to use the --amend flag with git commit, to avoid creating a new commit (which would create a new ChangeId in the commit text, and would be bad). Gerrit uses change IDs to uniquely represent changes, so if you create a new commit instead of amending a previous one, you will also create a new review thread in Gerrit instead of modifying your existing one.

\textbf{Another note:} We're using '-f' with git review to delete this branch, since it was only crated by git review for the purpose of responding to a given round of review.

\pagebreak

\section{Tools for web server development}

\subsection*{HTTP in (some) context:}
\includegraphics*[width=\linewidth]{HTTPinContext.png}

Things you will see:
\begin{itemize}
    \item MIME Types
    \item Header lines
    \item \textbackslash r\textbackslash n\textbackslash r\textbackslash n or (\textbackslash n\textbackslash n)
    \item Connection management
    \item Error codes 
    \item Probers 
\end{itemize}

\subsection{Command-line HTTP tools}

\subsubsection{Netcat (NC)}

\begin{verbatim}
    $ echo -e "GET / HTTP/1.0\n\n" | \ nc code.cs130.org 80
\end{verbatim}

Output:

\begin{verbatim}
    HTTP/1.1 400 Bad Request
    Server: nginx/1.24.0
    Date: Tue, 14 May 2024 03:41:55 GMT
    Content-Type: text/html
    Content-Length: 157
    Connection: close

    <html>
    <head><title>400 Bad Request</title></head>
    <body>
    <center><h1>400 Bad Request</h1></center>
    <hr><center>nginx/1.24.0</center>
    </body>
    </html>
\end{verbatim}

Pros:
\begin{itemize}
    \item Versatile
    \item Can easily craft malformed requests
\end{itemize}

Cons:
\begin{itemize}
    \item Must manually craft well-formed requests
    \item No SSL support
    \item Knows nothing of HTTP 
\end{itemize}

\subsubsection{Curl (curl)}

\begin{verbatim}
    $ curl -v http://code.cs130.org/ 
\end{verbatim}

Output:
\begin{verbatim}
    *   Trying 34.105.65.41:80...
    * Connected to code.cs130.org (34.105.65.41) port 80
    > GET / HTTP/1.1
    > Host: code.cs130.org
    > User-Agent: curl/8.4.0
    > Accept: */*
    > 
    < HTTP/1.1 301 Moved Permanently
    < Server: nginx/1.24.0
    < Date: Tue, 14 May 2024 03:44:15 GMT
    < Content-Type: text/html
    < Content-Length: 169
    < Connection: keep-alive
    < Location: https://code.cs130.org/
    < 
    <html>
    <head><title>301 Moved Permanently</title></head>
    <body>
    <center><h1>301 Moved Permanently</h1></center>
    <hr><center>nginx/1.24.0</center>
    </body>
    </html>
    * Connection #0 to host code.cs130.org left intact
\end{verbatim}

Pros:
\begin{itemize}
    \item Output can be concise
    \item Robust HTTP support 
    \item Easy to use 
\end{itemize}
Cons:
\begin{itemize}
    \item Geared towards end-users, not developers
    \item Output can be hard to parse 
\end{itemize}

\subsubsection{HTTPie (HTTP)}

\begin{verbatim}
    $ http GET http://cdoe.cs130.org/
\end{verbatim}

Output:
\begin{verbatim}
    HTTP/1.1 301 Moved Permanently
    Connection: keep-alive 
    Content-Length: 194 
    Content-Type: text/html 
    Date: Mon, f14 Jan 2019 03:13:04 GMT 
    Location: https://code.cs130.org/ 
    Server: nginx/1.13.0 (Ubuntu)
\end{verbatim}

Pros:
\begin{itemize}
    \item Output is very configurable
    \item Robust HTTP support
    \item Can specify request method 
\end{itemize}

Cons:
\begin{itemize}
    \item Requires more configuration than other tools
\end{itemize}

\subsection{Using real Browsers}

Early goal should be to support browser requests. This is tough on web servers. Need to implement many (all?) aspects of HTTP

\subsubsection*{Chrome DevTools}
\includegraphics*[width=\linewidth]{chromeDevTools1.png}
\includegraphics*[width=\linewidth]{chromeDevTools2.png}


\pagebreak

\section{Build Systems}

\textbf{Build: } Compile your code.

(source code $\Rightarrow$ executable binary)

\vspace*{1em}

\textbf{Deploy: } Get your code running

(Run an executable)

\textbf{LESSONS:}
\begin{enumerate}
    \item Builds should be one simple step.
    \begin{verbatim}
        $ ./build.sh
    \end{verbatim}
    \item Builds should be repeatable.

    Different engineers at different times should get the same output.
    \item Valuable things go in revision control, being able to build your project is valuable.
    
    Therefore, build scripts go in revision control.
\end{enumerate}

But bash scripts can be tricky to write, aren't very readable, and aren't easy to maintain. 

\subsection{Make}

\begin{center}
    \includegraphics*[width=0.7\linewidth]{MakeConfigParser.png}
\end{center}

\subsubsection*{Exposing intermediates in g++}

Compiling and link can be done in one step:
\begin{verbatim}
    $ g++ foo.cc -o foo
\end{verbatim}

Or in two steps:
\begin{verbatim}
    $ g++ -c foo.cc # produces foo.o (compilation)
    $ g++ foo.o -o foo $ linking
\end{verbatim}

\subsubsection*{Compiling}

\begin{center}
    \includegraphics*[width=0.7\linewidth]{compilation.png}
\end{center}

\subsubsection*{Linking}

\begin{center}
    \includegraphics*[width=0.7\linewidth]{linking.png}
\end{center}

\subsubsection*{Dependency graph}

\begin{center}
    \includegraphics*[width=0.7\linewidth]{dependencyGraph.png}
\end{center}

Intermediates can be reused:
\begin{center}
    \includegraphics*[width=0.7\linewidth]{reusingIntermediates.png}
\end{center}

\subsubsection*{Config parser Makefile, version 1}

\begin{verbatim}
  config_parser_main: config_parser_main.o config_parser.o
    g++ -o config_parser_main config_parser_main.o config_parser.o \
    -std=c++11 -Wall -Werror
  
  config_parser.o: config_parser.cc config_parser.h
    g++ -c config_parser.cc -std=c++11 -Wall -Werror
  
  config_parser_main.o: config_parser_main.cc config_parser.h
    g++ -c config_parser_main.cc -std=c++11 -Wall -Werror
  
\end{verbatim}

\subsubsection*{Config parser Makefile, version 2}

\begin{verbatim}
    CXXFLAGS=-std=c++11 -Wall -Werror
    config_parser_main: config_parser_main.o config_parser.o
        $(LINK.cc) $^ $(LDLIBS) -o $@

    config_parser.o: config_parser.cc config_parser.h
        $(CXX) $(CXXFLAGS) -c $<

    config_parser_main.o: config_parser_main.cc config_parser.h
        $(CXX) $(CXXFLAGS) -c $<    
  
\end{verbatim}

\subsubsection{Who makes the Makefiles?}

Makefiles can get real complex, real fast.

It is difficult to: 
\begin{itemize}
    \item encode logic into Makefile 
    \item detect compilers
    \item detect and build with installed packages 
\end{itemize}

Solution: Generate Makefiles

\subsection{GNU Build System}
\begin{itemize}
    \item Autoconf + Automake
    \item Notable files:
    \begin{itemize}
        \item Makefile.am
        \item configure 
        \item Makefile.in 
        \item config.h 
    \end{itemize}
    \item  Run:
    \begin{itemize}
        \item \$ ./configure 
        \item \$ make
    \end{itemize}
    \item Creates GNU-compatible Makefiles
\end{itemize}

\subsection{CMake}
\begin{itemize}
    \item Single tool
    \item Notable files:
    \begin{itemize}
        \item CMakeLists.txt
        \item build/
        \item A ton of generated files 
    \end{itemize}
    \item Run:
    \begin{itemize}
        \item \$ cd build
        \item \$ cmake .. 
        \item \$ make 
    \end{itemize}
    \item Supports multiple output formats
\end{itemize}

\subsubsection{CMake Examples}

Define library:
\begin{verbatim}
    add_library(config_parser src/config_parser.cc) # libconfig_parser.a
\end{verbatim}

Define executable:
\begin{verbatim}
    add_executable(config_parser_test tests/config_parser_test.cc) # config_parser_test 
    target_link_libraries(config_parser_test config_parser gtest_main) 
\end{verbatim}

Add GTest tests:
\begin{verbatim}
    gtest_discover_tests(config_parser_test WORKING_DIRECTORY 
        ${CMAKE_CURRENT_SOURCE_DIR}/tests)
\end{verbatim}

Add non-GTest tests:
\begin{verbatim}
    add_test(NAME web_test
        COMMAND ${CMAKE_CURRENT_SOURCE_DIR}/tests/web_test.sh -s "$<TARGET_FILE:webserver>"
        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/tests)
\end{verbatim}

Include external source package:
\begin{verbatim}
    add_subdirectory(/usr/src/googletest googletest)
\end{verbatim}

Include external installed(compiled) library:
\begin{verbatim}
    find_package(Boost 1.50 REQUIRED COMPONENTS signals system)
\end{verbatim}

\subsection{Build Systems Wishlist}

A good build system has many desirable features, unfortunately some are contradictory.

\subsubsection{Correct}

The build should be a faithful representation of the current state of your source code. What if a file has changed somewhere, bu the build system doesn't realize it?

\begin{itemize}
    \item Easy to happen in GNU Make
    \item make clean to the rescue
    \item GMake detects changes by timestamps. Hashing is slower, but more accurate.
\end{itemize}

The build should be as close as possible ot what you're going to release. (Very hard to achieve!)
\begin{itemize}
    \item Debug vs. optimized builds?
    \item Instrumented binaries (e.g. to find memory bugs, coverage)
    \item Extra logging?
\end{itemize}

\subsubsection{Hermetic}
Different users in different environments should get the same result.

\begin{itemize}
    \item What if you're on different OSes? 
    \item What if you're using different compilers?

    Do you check your compiler into version control? Almost never, but Google does.
    \item What if your shells are configured differently?
    
    E.g. Different .bashrc, different umask, etc. 
    \item Possible solutions:
    \begin{itemize}
        \item Build in a virtual machine
        \item Build in a Docker container
    \end{itemize}
\end{itemize}

\subsubsection{Flexible}

Want ability to:
\begin{itemize}
    \item Change compilation and linking options
    \item Support other languages 
    \item Support custom build rules 
\end{itemize}

\subsubsection{Easy to use}
\begin{itemize}
    \item Build in one step
    \item Easy to set up for your project 
    \item One obvious way to do things (Make fails here!)
\end{itemize}

\subsubsection{Automatable}

Builds should happen automatically, and frequently

What's best for your project?
\begin{itemize}
    \item Nightly?
    \item After every submit? ("continuous build")
    \item Whenever you save a file you're editing?
\end{itemize}

\subsubsection{Fast}

The more builds you want to do, the faster it needs to be.

\vspace*{1em}

Some practices to increase build speed:

Tell GMake to run 4 build commands at a tim (-j4)
\begin{verbatim}
    make -j4 
\end{verbatim}

\textbf{Use Build cluster:} Distributing the build process across multiple machines or nodes in a cluster.

\vspace*{1em}

Cache intermediates and only build what has changed.

\subsubsection{Manage dependencies automatically}

This is important for fast and scalable on our wishlist. 

CMake does this for you!

\subsubsection{Scalable}

Want to support:
\begin{itemize}
    \item large codebases
    \item many users  
\end{itemize}

\pagebreak

\section{Deployment}

Deployment is the process that takes you from development to production. 

\vspace*{1em}

\textbf{Development: } Works on my machine!

\vspace*{1em}

\textbf{Production: }
\begin{itemize}
    \item Server: Running "in the cloud"
    \item Application: Running on users' phones, computers, etc.
\end{itemize}

\textbf{Observations: }
\begin{itemize}
    \item Deployment requires packaging
    \item Deployment requires isolation and resource management 
\end{itemize}

Virtual machines provides both, but at high overhead... is there a better way?

\subsection{Aside on virtualization}

Virtualization is a widely-used term in CS, but hard to define.

Easiest to define as the antonym of "actual":
\begin{itemize}
    \item Virtual memory vs. actual memory
    \item Virtual machine vs. actual machine
\end{itemize}

Virtualization predates even computers. Turing machines are a virtual computer. 

Universal turing machine $\Rightarrow$ a way to run virtual turing machines 

\vspace*{1em}

\textbf{Examples of virtualization}
\begin{itemize}
    \item Virtual functions: specify an interface without an implementation
    \item Memory virtualization: Present a large address space independent of RAM 
    \item Storage virtualization: Make many disks on may computers appear as one 
    \item JVM: Consistent data, memory, computation model on different architectures
    \item Machine virtualization
\end{itemize}

\includegraphics*[width=\linewidth]{virtualizationOverhead.png}

\subsection{Containerization}

\begin{multicols}{2}
    \subsubsection*{Containerization is a restricted view of the underlying OS }
        \begin{itemize}
            \item See group of related processes $\rightarrow$
            \item Manage and limit resource usage $\rightarrow$
            \item Separate or restricted filesystem $\rightarrow$
            \item Restrict system calls $\rightarrow$
        \end{itemize}
    \subsubsection*{Linux Kernel magic}
        \begin{itemize}
            \item Namespaces
            \item cgroups
            \item OverlayFS
            \item seccomp
        \end{itemize}
\end{multicols}

\subsection{Docker}

Docker provides utilities and glue for Linux containerization:
\begin{itemize}
    \item A way to build filesystems in layers
    \item A package format (Basically, a filesystem and config)
    \item A package repository
\end{itemize}

If running Docker on Mac/Windows: Moby Linux VM

\subsubsection{Docker Image}
\begin{itemize}
    \item Snapshot of OS which is defined by commands and stacked in layers 
    \item Created by 'docker build' 
    \item Defined by Dockerfile 
    \item Base is another image, e.g. ubuntu:latest 
    \item Can be tagged with versions, e.g. :latest 
\end{itemize}

\subsubsection{Docker Container}
\begin{itemize}
    \item Running instance of an image 
    \item Created by 'docker run'
\end{itemize}

\subsubsection{Docker Volume}
\begin{itemize}
    \item Virtual disk image
    \item Created by:
    \begin{itemize}
        \item docker run -v/--mount
        \item docker volume
    \end{itemize}
    \item Persists across restarts 
    \item Shareable between containers
\end{itemize}

\subsubsection{Docker Bind mount}
\begin{itemize}
    \item Mount host FS in container
    \item Created by:
    \begin{itemize}
        \item docker run -v/--mount 
    \end{itemize}
    \item Share data between host and container 
    \item Volume that lives in host FS 
\end{itemize}

\subsubsection{Docker Port}
\begin{itemize}
    \item Expose ports inside container to:
    \begin{itemize}
        \item Host: use 127.0.01
        \item Internet: use 0.0.0.0
    \end{itemize}
    \item Defined in:
    \begin{itemize}
        \item Image(Docker file) For documentation
        \item Container(docker run) For implementation
    \end{itemize}
\end{itemize}

\subsubsection{Docker Examples}

\subsubsection*{base.Dockerfile}

\begin{verbatim}
    # Get the base Ubuntu image from Docker Hub
    FROM ubuntu:latest as base

    # Update the base image and install build environment
    RUN apt-get update && apt-get install -y \
        build-essential \
        cmake \
        ...
\end{verbatim}

\subsubsection*{Dockerfile}
\begin{verbatim}
    # Define builder stage
    FROM my-proj:base as builder

    # Share work directory
    COPY . /usr/src/project
    WORKDIR /usr/src/project/build

    # Build and test
    RUN cmake ..
    RUN make
    RUN ctest --output-on_failure  

    # Define deploy stage
    FROM ubuntu:latest as deploy

    # Copy server output binary to "."
    COPY --from=builder /usr/src/project/build/bin/webserver .

    # Copy config files
    COPY conf/* conf/

    # Expose port 80
    EXPOSE 80

    # Use ENTRYPOINT to specify the binary name
    ENTRYPOINT ["./webserver"]

    # Use CMD to specify arguments to ENTRYPOINT
    CMD ["conf/deploy.conf"]
  
\end{verbatim}

\subsubsection{Container registries}

\includegraphics*[width=\linewidth]{containerRegistries.png}

\subsubsection{Google Cloud Build}

Define a series of build steps in cloudbuild.yaml.

To start a build:
\begin{verbatim}
    gclouds builds submit
\end{verbatim}

The build will be performed in the cloud, hermetically with no context from previous builds.

\vspace*{1em}

Output:
\begin{itemize}
    \item Build logs 
    \begin{itemize}
        \item Console
        \item Web UI 
    \end{itemize}
    \item Build artifacts 
    \begin{itemize}
        \item container images 
        \item compiled outputs
    \end{itemize}
\end{itemize}

\vspace*{1em}

\textbf{Build steps: }

\includegraphics*[width=\linewidth]{buildsteps.png}

\textbf{Whole config: }

\includegraphics*[width=\linewidth]{wholeConfig.png}

\pagebreak

\section{Refactoring}

Refactoring is rewriting code to improve some property, without changing the functionality.

\vspace*{1em}

In this case, we are restructuring the code to be more testable. But could also be refactored to be more maintainable: divide up long functions, make class do fewer things.

\vspace*{1em}

Each change is small and incremental, so the changes are more likely to be safe. Best practice is to create tests ahead of time so the test can verify correct behavior after the changes.
\begin{itemize}
    \item Mocks can be valuable when you are the client of large classes you don't want to test.
    \item Costly because you have to change the code's structure materially, usually by adding interfaces.
    \item Code often ends up a bit cleaner, sometimes you need to poke ugly holes for testing.
\end{itemize}

\subsubsection*{Refactor Ex 1}

\includegraphics*[width=0.6\linewidth]{refactorEx1.png}

\begin{center}
    \begin{itemize}
        \item Extract method when a fucntion gets too long or has a useful internal Boundary
        \item We used this to extract ProcessRequest() and later HandleRequest()
    \end{itemize}
\end{center}

\includegraphics*[width=0.6\linewidth]{refactorEx2.png}

\begin{center}
    \begin{itemize}
        \item Introduce param object when a method's param list gets too long.
        \item Certain functions have many params\par
        Often triggered by dependency injection or tunability
        \item Can create a param object (also known as an options struct) to encapsulate these params
        \item Nice side effect: you can define defaults and have a bit more control over values before the function executes
    \end{itemize}
\end{center}

\includegraphics*[width=0.6\linewidth]{refactorEx3.png}

\begin{center}
    \begin{itemize}
        \item Replace magic number with symbolic constant.
        \item There is nothing worse than random numbers strewn around code that have no clear documentation. (WHY DID YOU PICK THAT NUMBER???)
        \item Instead, symbolic constants make code self-documenting
        \item Done for the buffer length.
    \end{itemize}
\end{center}

\includegraphics*[width=0.6\linewidth]{refactorEx4.png}

\begin{center}
    \begin{itemize}
        \item Replace Constructor with Factory Method when you want your constructor to be able to fail without using exceptions. (Exceptions in C++ are difficult to use correctly and often best avoided)
        \item Can also help if you want to hide which derived type is being returned based on the input values.
        \item Potential downside: forces you to allocate this object on the heap. Not usually a huge issue.
    \end{itemize}
\end{center}

\includegraphics*[width=0.6\linewidth]{refactorEx5.png}

\begin{center}
    \begin{itemize}
        \item Introduce expression builder when your object has a required call sequence; ie, first call these setup functions, then you can use the rest of the functions. 
        \item Separate setup/construction functions from the runtime functions. 
        \item More flexible than a factory.
    \end{itemize}
\end{center}


\pagebreak

\section{Static Analysis}

Static analysis is a process that inspects the code of your program without executing it directly. Often a control flow graph is built, although not required. Static analysis looks for patterns in that graph that represent likely problems.

\vspace*{1em}

The most commonly used ones are compiler warnings, type checking, and linters.

However, most people associate static analysis with a program (other than compiler) that inspects your code for classes of bugs. This checker is often trying to disprove the existence of certain classes bugs in your code. 

\vspace*{1em}

\subsection{Compiler warnings}

\includegraphics*[width=\linewidth]{staticAnalysis.png}
\includegraphics*[width=\linewidth]{staticAnalysis2.png}

\subsection{Static type checking}

Static type checking gives you compile time errors about illegal operations. This is opposed to runtime typed languages that only give you errors at runtime.

You can even do fancy things like type inference (auto in C++x11) at compile time.

\subsection{Linters}

Lint was a tool originally developed alongside the C programming language,  intended to help catch non-portable constructs.

You'll often find a less pedantic linter built into compiler frontends.

\begin{verbatim}
    $ clang-tidy test.cc
\end{verbatim}

Linters may not seem like static analysis, but they correct more than just style.

Here are some examples:
\begin{itemize}
    \item Inaccurate erase/remove 
    \item Suspicious semicolon 
    \item Unused RAII 
    \item Use after move 
\end{itemize}

\begin{center}
    \includegraphics*[width=0.7\linewidth]{linter.png}
\end{center}

\subsection{Examples}

\subsubsection{Use after free}

\begin{verbatim}
    // Allocate 'a' on the heap
    int* a = new int;
    
    // 'a' is still live, so this is OK
    *a = 7;
    
    // Free memory associated with 'a'
    delete a;
    
    // Best case, this will trigger a SIGSEGV.
    // Worst case, this will not trigger a SIGSEGV and silently
    // do the wrong thing.
    *a = 8;
    
\end{verbatim}

We can statically determine that 'a' isn't live on the last line

Value:
\begin{itemize}
    \item Helps us avoid SIGSEGVs
    \item Helps avoid attacks that may be able to run malicious code by taking advantage of a use-after free 
\end{itemize}

(trivial example)

\hrulefill

\begin{verbatim}
    class Foo {
        public:
         Foo(int* a) a_(a) { … }
         set(int a) { *a_ = a; }
        private:
         int* a_;
       };
       
       Foo build() {
         int a;
         return Foo(&a);
       }
       
       void run() {
         // Best case, this will trigger a SIGSEGV.
         // Worst case, this silently do the wrong thing.
         build().set(7);
       }
       
\end{verbatim}

In this case, \textit{int a} isn't live at the point where it will be used in \textit{run()}

\vspace*{1em}

This is because \textit{Foo} stored a pointer to \textit{int a} as a member,

and then \textit{int a} was freed when it went out of scope at the end of \textit{build()}.

\subsubsection{Buffer overrun}

\begin{verbatim}
    // Wrong:
    int a[10];
    memset(a, 0, 100);
    // This just stomped on 90*4 bytes past the end of 'a'.
    
    // Right:
    int a[10];
    memset(a, 0, sizeof(a));

    // Better:
    int a[10] = {};

\end{verbatim}

Reading or writing past the end of a buffer will produce undefined results.

\vspace*{1em}

Hopefully you run into a guard page and it causes a SIGSEGV, otherwise it will just silently stomp on memory. 

\vspace*{0.5em}

The compiler can often catch this when the size is known at compile time, but it is much harder if it is dynamically sized.

\subsubsection{Deadlock Detection}

\begin{verbatim}
    Mutex mu1, mu2;

    void foo() {
      mu1.Lock();
      mu2.Lock();
      mu2.Unlock();
      mu1.Unlock();
    }
    
    void bar() {
      mu2.Lock();
      mu1.Lock();
      mu1.Unlock();
      mu2.Unlock();
    }
    
    // It is not safe to run foo() and bar() concurrently    
\end{verbatim}

Deadlock is when you have 2+ routines waiting on each other in a cycle. Order resource acquisition is one of a few ways to avoid deadlock. 

\vspace*{1em}

Static analyzers can detect if there is a reachable state where both \textit{foo()} and \textit{bar} are both concurrently executing and waiting on each other.

\hrulefill

\begin{verbatim}
    Mutex mu1, mu2;
    int a GUARDED_BY(mu1);
    int b GUARDED_BY(mu2);
    
    void foo() REQUIRES(mu1, mu2) {
      a = 0;
      b = 0;
    }
    
    void test() {
      mu1.Lock();
      foo();         // Warning!  Requires mu2.
      mu1.Unlock();
    }    
\end{verbatim}

\textbf{Lock annotation:} You can sometimes annotate your code to help the static analyzer better understand the intended behavior. In the case of locks, you can explain what mutex guard which vars and then the static analyzer can check that invariant.

\subsubsection{Uninitialized variable}

\begin{verbatim}
    // Wrong:
    int a;
    printf(“%d\n”, a);
    // This printed random garbage from the stack.

    // Right:
    int a = 0;
    printf(“%d\n”, a);
\end{verbatim}

Uninitialized vars can result in undefined behavior. Assign-before-use is often easily identified by compilers or other static checks.

\subsubsection{Dead code}

\begin{verbatim}
    const int kConstant = 7;

    int foo = 5;
    if (foo > kConstant) {
      // All this code is dead...
      [...]
    }
    
\end{verbatim}

It is possible to prove that certain blocks are never reachable. This is a contrived example, but can be arbitrarily complex. In general, you can attempt to determine if a guard will always be false. 

\subsection{How static analysis works}

In general, static analysis can be reduced to the halting problem, so is undecidable in general. 

\vspace*{1em}

But we can produce approximates, and require assumptions/annotation to assist.

\vspace*{1em}

There are two typical approaches:
\begin{enumerate}
    \item Abstract interpretation
    \item Data flow analysis
\end{enumerate}

\subsubsection{Abstract implementation}
Abstract implementation: model effect of statements on an abstract machine to identify mistakes

\begin{enumerate}
    \item Walk the control flow graph 
    \item Keep track of things that are true when a given unit of code executes 
    \item Determine if invariants are broken 
    \item Example bugs you catch:
    \begin{itemize}
        \item Use after free 
        \item Uninitialized vars 
        \item Deadlock (if include concurrent execution)
    \end{itemize}
\end{enumerate}

\includegraphics*[width=0.8\linewidth]{abstractInterpretation.png}

\subsubsection{Data Flow Analysis}
Data flow analysis: attempt to determine possible input values based on the control flow graph (similar to general type inference in languages like ocaml)

\begin{enumerate}
    \item Keep track of the set of possible values a variable can take at a given point in the program. 
    \item Identify statements that break invariants for a possible value a variable could take on that point.
    \item Example bugs:
    \begin{itemize}
        \item buffer overrun 
        \item dead/unreachable code 
    \end{itemize}
\end{enumerate}

\includegraphics*[width=0.7\linewidth]{dataFlowAnalysis.png}

\subsection{Limitation of Static Analysis}

There are an exponential number of paths through a program, so keeping track of each requires an exponential amount of memory.

\vspace*{1em}

The range of possible inputs isn't limited much throughout the program, which results in false positives.

\vspace*{1em}

To overcome these limitations, we often have to pick between low recall (too few bugs get caught) and low precision (correct code gets erroneously flagged).

\vspace*{1em}

\hrulefill

One solution is extend the language. One example is Java's \textit{@VisibleForTesting} annotation. 

\vspace*{0.5em}

In Java, best practices dictate class methods should be private unless they need to be used outside of that class. Static analysis can flag when methods are non-private but also unused outside of class context. 

\vspace*{0.5em}

So if you want to access methods for unit testing purposes, make the method public for use by the unit test. Maintain Java best practices and act like the method is private by ensuring it is not used anywhere else in the program.

\hrulefill

Another solution: Introduce new syntax 

\vspace*{.5em}

Define "Typescript", which is a superset of Javascript that can provide detailed typing information.

\vspace*{.5em}

Then provide tools:
\begin{itemize}
    \item Tool to perform static analysis 
    \item Tool to automatically generate Javascript from Typescript 
\end{itemize}

In reality, this is the same tool: \textbf{The Typescript Compiler}

\pagebreak

\section{Logging and exception handling}

\textbf{Why log? }
\begin{itemize}
    \item Development
    \item Debugging
    \item Security
    \item Monitoring 
    \item Usage Insights
\end{itemize}

\vspace*{2em}

\textbf{How to log?}
\begin{verbatim}
    #include <boost/log/trivial.hpp>
    #include <iostream>
    
    int main(int, char *[]) {
      BOOST_LOG_TRIVIAL(info) << "This is some info";
    
      BOOST_LOG_TRIVIAL(warning) << "Not great...";
    
      BOOST_LOG_TRIVIAL(error) << "OH NOOOO";
    
      BOOST_LOG_TRIVIAL(fatal) << "eff this I'm out";
    
      return 0;
    }    
\end{verbatim}

\textbf{When to log?}

\textit{Whenever something interesting things happen!} E.g. Ready to serve, serving a request, errors, shutting down

\vspace*{1em}

\textbf{What to log?}

Verify correct usage, and detect abuse.

For example this might be logged for each request:
\begin{itemize}
    \item Timestamp 
    \item Thread ID 
    \item IP of request 
    \item Severity
    \item .... etc.
\end{itemize}

\vspace*{1em} 

\textbf{Where to log?}

The goals for statement logs are:
\begin{itemize}
    \item Inspect manually 
    \item Parse with tools 
    \item Do not fill disk 
    \item Persist after restart/crash 
    \item Resilient to hardware failures 
\end{itemize}

We can keep them in log sinks such as:
\begin{itemize}
    \item Console stdout/stderr 
    \item File(s) on disk 
    \item Remote servers 
\end{itemize}

To avoid filling up the disk, rotate log files. 

E.g. to cap usage at 100MB:
\begin{itemize}
    \item When log file > 10MB: 
    \begin{itemize}
        \item move logname.log to logname\_YYYYMMDDhhmm.log 
        \item Delete oldest log if 10 or more exist 
    \end{itemize}
\end{itemize}

\includegraphics*[width=\linewidth]{logsDockerCloud.png}

\subsection{Notes on Severity}
\textbf{Severity usually multi-level:}
\begin{itemize}
    \item trace/verbose/debug 
    \item info 
    
    just some info 
    \item warning 

    Not great.... 
    \item error 

    OH NOOOO 
    \item fatal 

    eff this im out
\end{itemize}
Specify minimum severity level for log destinations: logLevel=info includes info, warning, and error 



\pagebreak

\end{document}